{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1U2ZT9SJfZ25YZ4DTbhOjNL2d7IHngb39",
      "authorship_tag": "ABX9TyMVUwFtWGYnzKNJM2LpMUb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leandrohbar/Machine_learning/blob/main/Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "titanicTrain = '/content/drive/MyDrive/Arquivos CSV/TITANIC/train.csv'\n",
        "titanicTest = '/content/drive/MyDrive/Arquivos CSV/TITANIC/test.csv'\n",
        "submission = '/content/drive/MyDrive/Arquivos CSV/TITANIC/gender_submission.csv'\n",
        "\n",
        "train_data = pd.read_csv(titanicTrain)\n",
        "test_data = pd.read_csv(titanicTest)\n",
        "sub_file = pd.read_csv(submission)"
      ],
      "metadata": {
        "id": "-y7Cq4ohMNMq"
      },
      "execution_count": 401,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "lxTS2hEMQjs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "id": "rV7Hsgm5QnBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_ticket_data(data_frame):\n",
        "    \"\"\"\n",
        "    Process ticket-related data in the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        data_frame (DataFrame): The DataFrame containing ticket-related columns.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A copy of the input DataFrame with ticket-related data processed.\n",
        "    \"\"\"\n",
        "    data_frame = data_frame.copy()\n",
        "\n",
        "    # Define a function to normalize names\n",
        "    def normalize_name(x):\n",
        "        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n",
        "\n",
        "    # Define a function to extract ticket numbers\n",
        "    def extract_ticket_number(x):\n",
        "        return x.split(\" \")[-1]\n",
        "\n",
        "    # Define a function to extract ticket items\n",
        "    def extract_ticket_item(x):\n",
        "        items = x.split(\" \")\n",
        "        if len(items) == 1:\n",
        "            return \"None\"\n",
        "        return \" \".join(items[0:-1])\n",
        "\n",
        "    # Apply the normalization function to the 'Name' column\n",
        "    data_frame['Name'] = data_frame['Name'].apply(normalize_name)\n",
        "\n",
        "    # Extract and create a new column for ticket numbers\n",
        "    data_frame['Ticket_number'] = data_frame['Ticket'].apply(extract_ticket_number)\n",
        "\n",
        "    # Extract and create a new column for ticket items\n",
        "    data_frame['Ticket_item'] = data_frame['Ticket'].apply(extract_ticket_item)\n",
        "\n",
        "    return data_frame\n",
        "\n",
        "# Process ticket-related data for the train and test DataFrames\n",
        "train_d = process_ticket_data(train_data)\n",
        "test_d = process_ticket_data(test_data)\n"
      ],
      "metadata": {
        "id": "9L3Z4L2DRG2F"
      },
      "execution_count": 402,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_d.head()"
      ],
      "metadata": {
        "id": "OQKMAtV0n8s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ua-2xRxdpIn9"
      },
      "execution_count": 403,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Input Features\n",
        "\n",
        "# Get a list of all columns in the 'train_d' DataFrame\n",
        "inputFeatures = list(train_d.columns)\n",
        "\n",
        "# Remove the 'Ticket' column from the list of input features\n",
        "inputFeatures.remove('Ticket')\n",
        "\n",
        "# Remove the 'PassengerId' column from the list of input features\n",
        "inputFeatures.remove('PassengerId')\n",
        "\n",
        "# Remove the 'Survived' column from the list of input features\n",
        "inputFeatures.remove('Survived')\n",
        "inputFeatures.remove('Name')"
      ],
      "metadata": {
        "id": "e8sCaK9aqIpq"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Input Features (X) and Target Variable (y)\n",
        "\n",
        "# 'inputFeatures' is a list of column names containing the input features\n",
        "X = train_d[inputFeatures]\n",
        "X_test = test_d[inputFeatures]\n",
        "# 'Survived' is the target variable you want to predict\n",
        "y = train_d['Survived']"
      ],
      "metadata": {
        "id": "NfN4b2FIrAtI"
      },
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values and One-Hot Encoding\n",
        "\n",
        "# Import the required libraries\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Perform one-hot encoding for categorical features in the training data\n",
        "X_encoded = pd.get_dummies(X, columns=['Sex', 'Cabin', 'Embarked', 'Ticket_number', 'Ticket_item'])\n",
        "\n",
        "# Perform one-hot encoding for categorical features in the test data\n",
        "X_test_encoded = pd.get_dummies(X_test, columns=['Sex', 'Cabin', 'Embarked', 'Ticket_number', 'Ticket_item'])\n",
        "\n",
        "# Identify columns with some missing values\n",
        "columns_with_some_nulls = X_encoded.columns[X_encoded.isnull().any()]\n",
        "columns_with_some_nulls_test = X_test_encoded.columns[X_test_encoded.isnull().any()]\n",
        "\n",
        "# Create a SimpleImputer with the 'mean' strategy\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fill missing values in the training data with the mean of respective columns\n",
        "X_encoded[columns_with_some_nulls] = imputer.fit_transform(X_encoded[columns_with_some_nulls])\n",
        "\n",
        "# Fill missing values in the test data with the same imputer\n",
        "X_test_encoded[columns_with_some_nulls_test] = imputer.fit_transform(X_test_encoded[columns_with_some_nulls_test])\n",
        "\n",
        "# Display information about the encoded training and test data\n",
        "X_encoded.info()\n",
        "X_test_encoded.info()\n"
      ],
      "metadata": {
        "id": "trv41lZTyn-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding and Dropping Different Columns\n",
        "\n",
        "# Get the column lists from X_encoded and X_test_encoded\n",
        "colunas_df1 = X_encoded.columns.tolist()\n",
        "colunas_df2 = X_test_encoded.columns.tolist()\n",
        "\n",
        "# Find the columns that are in df1 but not in df2\n",
        "colunas_diferentes_df2 = [coluna for coluna in colunas_df2 if coluna not in colunas_df1]\n",
        "\n",
        "# Drop the different columns from X_test_encoded\n",
        "X_test_encoded = X_test_encoded.drop(colunas_diferentes_df2, axis=1)\n",
        "\n",
        "# Find the columns that are in df2 but not in df1\n",
        "colunas_df1 = X_encoded.columns.tolist()\n",
        "colunas_df2 = X_test_encoded.columns.tolist()\n",
        "\n",
        "colunas_diferentes_df1 = [coluna for coluna in colunas_df1 if coluna not in colunas_df2]\n",
        "\n",
        "# Drop the different columns from X_encoded\n",
        "X_encoded = X_encoded.drop(colunas_diferentes_df1, axis=1)\n"
      ],
      "metadata": {
        "id": "WLVU7VRNShTh"
      },
      "execution_count": 410,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the Data into Training and Validation Sets\n",
        "\n",
        "# Split the input features (X) and target variable (y) into training and validation sets.\n",
        "# The 'random_state' parameter ensures reproducibility of the split.\n",
        "train_X, aval_X, train_y, aval_y = train_test_split(X_encoded, y, random_state=1)\n"
      ],
      "metadata": {
        "id": "SwGn7P_arPe_"
      },
      "execution_count": 411,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Calculate Mean Absolute Error (MAE) for DecisionTreeRegressor with Threshold\n",
        "\n",
        "def get_mae(max_leaf_nodes, train_X, aval_X, train_y, aval_y):\n",
        "    # Create a DecisionTreeRegressor model with the specified max_leaf_nodes\n",
        "    model_train = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    model_train.fit(train_X, train_y)\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    predict = model_train.predict(aval_X)\n",
        "\n",
        "    # Define a threshold for binary classification\n",
        "    threshold = 0.5\n",
        "\n",
        "    # Convert predictions to binary values based on the threshold\n",
        "    predict = [1 if p > threshold else 0 for p in predict]\n",
        "\n",
        "    # Calculate the Mean Absolute Error (MAE) between predictions and actual values\n",
        "    mae = mean_absolute_error(predict, aval_y)\n",
        "\n",
        "    return mae\n",
        "\n",
        "# Define a list of max_leaf_nodes values to test\n",
        "max_leaf_nodes = [50, 65, 70, 90, 95, 99, 100, 110, 120]\n",
        "\n",
        "# Calculate MAE for each max_leaf_nodes value and store it in a dictionary\n",
        "scores = {leaf_size: get_mae(leaf_size, train_X, aval_X, train_y, aval_y) for leaf_size in max_leaf_nodes}\n",
        "\n",
        "# Find the best max_leaf_nodes value with the minimum MAE\n",
        "best_tree_size = min(scores, key=scores.get)\n"
      ],
      "metadata": {
        "id": "7DCFHVkMsy2k"
      },
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Final Decision Tree Regressor Model\n",
        "\n",
        "# Create a DecisionTreeRegressor model with the best max_leaf_nodes value\n",
        "final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n",
        "\n",
        "# Fit the final model on the fully preprocessed training data (X_encoded) and target labels (y)\n",
        "final_model.fit(X_encoded, y)\n"
      ],
      "metadata": {
        "id": "X3WFZhVwGx3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Predictions with the Final Model and Calculating MAE\n",
        "\n",
        "# Make predictions using the final model on the preprocessed training data\n",
        "final_predict = final_model.predict(X_encoded)\n",
        "\n",
        "# Define a threshold for binary classification\n",
        "threshold = 0.5\n",
        "\n",
        "# Convert the continuous predictions to binary values based on the threshold\n",
        "final_predict = [1 if p > threshold else 0 for p in final_predict]\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE) between the final predictions and actual target labels\n",
        "mae_ = mean_absolute_error(final_predict, y)\n"
      ],
      "metadata": {
        "id": "8pixlbi_Hjzb"
      },
      "execution_count": 423,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Predictions on Test Data with the Final Model\n",
        "\n",
        "# Make predictions using the final trained model (final_model) on the preprocessed test data (X_test_encoded)\n",
        "survived_final = final_model.predict(X_test_encoded)\n",
        "\n",
        "# Define a threshold for binary classification\n",
        "threshold = 0.5\n",
        "\n",
        "# Convert the continuous predictions to binary values based on the threshold\n",
        "survived_final = [1 if p > threshold else 0 for p in survived_final]\n"
      ],
      "metadata": {
        "id": "TXO-mzwwIVO6"
      },
      "execution_count": 422,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_file['Survived'] = survived_final"
      ],
      "metadata": {
        "id": "5W25mDF9WdkN"
      },
      "execution_count": 419,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_file.head()"
      ],
      "metadata": {
        "id": "ZhOpnPMxYpNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_file.to_csv('Submission.csv', index=False)"
      ],
      "metadata": {
        "id": "_yiVjeJddeSS"
      },
      "execution_count": 421,
      "outputs": []
    }
  ]
}